{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import functools\n",
    "import json\n",
    "import operator\n",
    "import sys\n",
    "from traceback import format_exc\n",
    "\n",
    "import pyspark\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql import Row, SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from kpi.zte import *\n",
    "from udf_formula_zte import udf_formula\n",
    "\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "\n",
    "sys.path.append('./kpi/zte/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/14 08:26:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .config(\"spark.jars\", \"/Users/khqp/Documents/GitHub/spark/drivers/postgresql-42.5.4.jar\")\\\n",
    "    .config(\"spark.executor.extraClassPath\", \"/Users/khqp/Documents/GitHub/spark/drivers/postgresql-42.5.4.jar\") \\\n",
    "    .master('local') \\\n",
    "    .appName(\"PySpark_Postgres_test\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "    # .schema(T.StructType([\n",
    "    #     T.StructField('id',T.StringType()),\n",
    "    #     T.StructField('enodeb_id', T.StringType()),\n",
    "    #     T.StructField('cell_id',T.StringType()),\n",
    "    #     T.StructField('duration', T.IntegerType()),\n",
    "    #     T.StructField('counters', T.D()),\n",
    "    #     ]))\\\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_option = {\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "    # \"url\": \"jdbc:postgresql://localhost:5432/pasti\",\n",
    "    \"url\": \"jdbc:postgresql://172.31.2.134:5432/kso_zte_dev\",\n",
    "    \"user\": \"ossdev\",\n",
    "    \"password\": \"datasintesa2023\"\n",
    "}\n",
    "\n",
    "df_option_master = {\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "    # \"url\": \"jdbc:postgresql://localhost:5432/pasti\",\n",
    "    \"url\": \"jdbc:postgresql://172.31.1.158:5000/oss_master\",\n",
    "    \"user\": \"oss_master_user\",\n",
    "    \"password\": \"user_master_oss\"\n",
    "}\n",
    "\n",
    "df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .options(**df_option)\n",
    "    \n",
    "\n",
    "df_master = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .options(**df_option_master)\n",
    "    \n",
    "df_raw = df.option(\n",
    "    'query', \"\"\"SELECT * FROM raw_counter WHERE processed = False LIMIT 1000\"\"\").load()\n",
    "df_site = df_master.option(\"dbtable\", \"sites\").load()\n",
    "\n",
    "# df_site.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "@F.udf\n",
    "def sum_json(x): \n",
    "    result = dict(functools.reduce(operator.add, map(collections.Counter, map(lambda a : json.loads(a), x))));\n",
    "    return json.dumps(result)\n",
    "\n",
    "\n",
    "data_raw = df_raw \\\n",
    "    .alias('raw_counter') \\\n",
    "    .join(df_site, df_raw.enodebId == df_site.enodebId, 'inner') \\\n",
    "    # .select(F.col('raw_counter.id'), F.col(\"cellId\"), F.col(\"duration\"), F.col(\"counters\"), F.col(\"startedAt\"),   F.col(\"siteId\")) \\\n",
    "        \n",
    "execute_ids = data_raw.rdd.map(lambda x: x[0]).collect()\n",
    "        \n",
    "data_select = data_raw \\\n",
    "    .select(F.col(\"cellId\"), F.col(\"duration\"), F.col(\"counters\"), F.col(\"startedAt\"),   F.col(\"siteId\")) \\\n",
    "\n",
    "data_c_level = data_select \\\n",
    "    .withColumn('kpi', udf_formula(F.col('counters'),  F.col('duration'))) \\\n",
    "    # .printSchema()\n",
    "    # .collect()\n",
    "    # .show(5)\n",
    "\n",
    "data_site_level = data_select \\\n",
    "    .groupBy(['siteId', 'startedAt']) \\\n",
    "    .agg(sum_json(F.collect_list('counters')).alias('counters'), F.sum('duration').alias('duration'), F.lit('-').alias('cellId')) \\\n",
    "    .withColumn('kpi', udf_formula(F.col('counters'),  F.col('duration'))) \\\n",
    "    # .printSchema()\n",
    "\n",
    "    # .collect()\n",
    "    # .show(5)\n",
    "    \n",
    "# print(data_c_level, data_site_level)\n",
    "\n",
    "data = data_c_level.unionByName(data_site_level).rdd.map(lambda x: (\n",
    "    x['siteId'],\n",
    "    x['cellId'],\n",
    "    x['duration'],\n",
    "    x['counters'],\n",
    "    x['startedAt'],\n",
    "    x['kpi'],\n",
    ")).collect()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_mapping = {\n",
    "    \"enodeb_id\": 'siteId',\n",
    "    \"cell_id\": 'cellId',\n",
    "    \"duration\": \"duration\",\n",
    "    \"counters\": \"counters\",\n",
    "    \"started_at\": \"startedAt\",\n",
    "    \"kpi\": \"kpiValue\",\n",
    "}\n",
    "\n",
    "df_write_option = {\n",
    "    **df_option,\n",
    "    \"batchSize\" : 1000,\n",
    "    \"dbtable\": \"site_kpi\",\n",
    "    \"createTableOptions\": \"\"\"\n",
    "                            ON CONFLICT (\"siteId\", \"cellId\", \"startedAt\") \n",
    "                            DO UPDATE SET counters = EXCLUDED.counters,\n",
    "                                \"kpiValue\" = EXCLUDED.\"kpiValue\",\n",
    "                                \"updatedAt\" = now()\n",
    "                        \"\"\"\n",
    "}\n",
    "\n",
    "# values = []\n",
    "# execute_ids = []\n",
    "\n",
    "# for x in data : \n",
    "#     # execute_ids.append(x['id'])\n",
    "#     values.append((\n",
    "#         x['siteId'],\n",
    "#         x['cellId'],\n",
    "#         x['duration'],\n",
    "#         x['counters'],\n",
    "#         x['startedAt'],\n",
    "#         x['kpi'],\n",
    "#     ))\n",
    "\n",
    "# print(tuple(execute_ids))\n",
    "\n",
    "conn = psycopg2.connect('postgresql://postgres:root@localhost:5432/pasti')\n",
    "conn_kso = psycopg2.connect(\n",
    "    'postgresql://ossdev:datasintesa2023@172.31.2.134:5432/kso_zte_dev')\n",
    "\n",
    "try:\n",
    "    cur = conn.cursor()\n",
    "    cur_kso = conn_kso.cursor()\n",
    "    query = \"\"\"\n",
    "        INSERT INTO site_kpi (\"siteId\", \"cellId\", \"duration\", \"counters\", \"startedAt\", \"kpiValue\")\n",
    "        VALUES %s\n",
    "        ON CONFLICT (\"siteId\", \"cellId\", \"startedAt\")\n",
    "        DO UPDATE SET \"kpiValue\" = EXCLUDED.\"kpiValue\", \"counters\" = EXCLUDED.\"counters\", \"updatedAt\" = now()\n",
    "    \"\"\"\n",
    "            \n",
    "    query_update_raw = \"\"\"\n",
    "        UPDATE raw_counter\n",
    "        SET processed = True\n",
    "        WHERE id IN %s\n",
    "    \"\"\"\n",
    "    # print(cur_kso.mogrify(query_update_raw, (tuple(execute_ids),)))\n",
    "    execute_values(cur, query, data, page_size=1000)\n",
    "    # cur_kso.execute(query_update_raw, (tuple(execute_ids),))\n",
    "    \n",
    "    conn.commit()\n",
    "    # conn_kso.commit()\n",
    "   \n",
    "except Exception as e:\n",
    "    print(format_exc())\n",
    "    conn.rollback()\n",
    "    conn_kso.rollback()\n",
    "finally : \n",
    "    conn.close()\n",
    "    conn_kso.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0c4f7ce70975b4d3930825d60f4f31d20a740b6b1150d514777d707e7b09658b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
